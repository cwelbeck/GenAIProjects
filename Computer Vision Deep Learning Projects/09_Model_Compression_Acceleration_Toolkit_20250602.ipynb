{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe06e4b",
   "metadata": {},
   "source": [
    "# ⚙️ Project: Model Compression + Acceleration Toolkit\n",
    "This notebook shows how to apply quantization and pruning to a PyTorch image classification model, followed by exporting to ONNX format for efficient inference on edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba2f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization\n",
    "from torchvision import models\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98033e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "print('Model quantized!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "pruned_model = copy.deepcopy(quantized_model)\n",
    "for name, module in pruned_model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
    "print('Model pruned!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20002c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy input for ONNX export\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "onnx_path = 'compressed_resnet18.onnx'\n",
    "torch.onnx.export(pruned_model, dummy_input, onnx_path,\n",
    "                  input_names=['input'], output_names=['output'],\n",
    "                  opset_version=11)\n",
    "print(f'Model exported to ONNX format at {onnx_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe083bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "inputs = {session.get_inputs()[0].name: dummy_input.numpy()}\n",
    "outputs = session.run(None, inputs)\n",
    "print('ONNX inference successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a63a3",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "We applied model compression techniques including quantization and pruning to ResNet18, and exported the model to ONNX for deployment. You can extend this by:\n",
    "- Converting to TensorRT or OpenVINO for further acceleration.\n",
    "- Profiling latency and memory usage.\n",
    "- Packaging into a CLI or API."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
