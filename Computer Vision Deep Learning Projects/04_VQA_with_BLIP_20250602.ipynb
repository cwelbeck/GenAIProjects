{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab181f7",
   "metadata": {},
   "source": [
    "# ðŸ¤– Project: Visual Question Answering (VQA) System\n",
    "This notebook demonstrates how to use a pretrained BLIP model for Visual Question Answering. We input an image and a question, and the model predicts the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898be630",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate timm Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8707c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP processor and model\n",
    "processor = BlipProcessor.from_pretrained('Salesforce/blip-vqa-base')\n",
    "model = BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-base')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ecb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image from URL\n",
    "img_url = 'https://raw.githubusercontent.com/salesforce/BLIP/main/demo/BLIP_demo.jpg'\n",
    "image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e05f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a question\n",
    "question = 'What is the man doing?'\n",
    "inputs = processor(image, question, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eedb13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the answer\n",
    "with torch.no_grad():\n",
    "    out = model(**inputs)\n",
    "    answer = processor.tokenizer.decode(out.logits.argmax(-1).squeeze(), skip_special_tokens=True)\n",
    "    print(f'Q: {question}\\nA: {answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed90d3e9",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "We used a BLIP model from Hugging Face to perform VQA. You can further extend this by:\n",
    "- Using your own dataset with custom Q&A.\n",
    "- Creating a Flask or Gradio app interface.\n",
    "- Combining with OCR or multilingual support."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
