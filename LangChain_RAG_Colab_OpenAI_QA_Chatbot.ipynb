{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udde0 LangChain RAG Chatbot with Gradio\n", "Upload your document (.txt, .csv, .pdf) and ask natural language questions."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["!pip install gradio langchain langchain_community faiss-cpu openai tiktoken chromadb unstructured sentence-transformers"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import gradio as gr\n", "from tempfile import NamedTemporaryFile\n", "from langchain_community.document_loaders import TextLoader, CSVLoader, PyPDFLoader\n", "from langchain.text_splitter import RecursiveCharacterTextSplitter\n", "from langchain.vectorstores import FAISS\n", "from langchain.embeddings import HuggingFaceEmbeddings\n", "from langchain.chains import RetrievalQA\n", "from langchain_community.chat_models import ChatOpenAI\n", "import os\n", "\n", "qa_chain = None\n", "chat_history = []\n", "\n", "def process_file(file):\n", "    global qa_chain, chat_history\n", "    chat_history = []\n", "    with NamedTemporaryFile(delete=False, suffix=file.name.split('.')[-1]) as tmp:\n", "        tmp.write(file.read())\n", "        path = tmp.name\n", "\n", "    if path.endswith(\".txt\"):\n", "        documents = TextLoader(path).load()\n", "    elif path.endswith(\".csv\"):\n", "        documents = CSVLoader(file_path=path).load()\n", "    elif path.endswith(\".pdf\"):\n", "        documents = PyPDFLoader(path).load()\n", "    else:\n", "        return \"Unsupported file format\"\n", "\n", "    chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(documents)\n", "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n", "    vectorstore = FAISS.from_documents(chunks, embeddings)\n", "    retriever = vectorstore.as_retriever()\n", "\n", "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n", "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n", "    return \"\u2705 Document processed!\""]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def chat_with_bot(messages):\n", "    if qa_chain is None:\n", "        return messages + [{\"role\": \"assistant\", \"content\": \"\u274c Please upload a document first.\"}]\n", "    query = messages[-1][\"content\"]\n", "    response = qa_chain.run(query)\n", "    return messages + [{\"role\": \"assistant\", \"content\": response}]\n", "\n", "with gr.Blocks() as demo:\n", "    gr.Markdown(\"# \ud83e\udde0 RAG Chatbot with Document Upload\")\n", "    file_input = gr.File(label=\"Upload a .txt, .csv, or .pdf\")\n", "    status = gr.Textbox(label=\"Status\")\n", "    file_input.change(fn=process_file, inputs=file_input, outputs=status)\n", "    gr.ChatInterface(fn=chat_with_bot, title=\"LangChain RAG Chatbot\", chatbot=gr.Chatbot(type=\"messages\"))\n", "demo.launch(share=True)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 2}