{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaqWr9s0Pqrz"
      },
      "outputs": [],
      "source": [
        "pip install virtualenv\n",
        "virtualenv my_env # create a virtual environment named my_env\n",
        "source my_env/bin/activate # activate my_env"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing necessary pacakges in my_env\n",
        "python3.11 -m pip install \\\n",
        "gradio==4.44.0 \\\n",
        "ibm-watsonx-ai==1.1.2 \\\n",
        "langchain==0.2.11 \\\n",
        "langchain-community==0.2.10 \\\n",
        "langchain-ibm==0.1.11"
      ],
      "metadata": {
        "id": "aLvOBeWbPyMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create gradio_demo.py\n",
        "touch gradio_demo.py"
      ],
      "metadata": {
        "id": "2vcq9FWHQnWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradio_demo.py\n",
        "import gradio as gr\n",
        "\n",
        "def add_numbers(Num1, Num2):\n",
        "    return Num1 + Num2\n",
        "\n",
        "# Define the interface\n",
        "demo = gr.Interface(\n",
        "    fn=add_numbers,\n",
        "    inputs=[gr.Number(), gr.Number()], # Create two numerical input fields where users can enter numbers\n",
        "    outputs=gr.Number() # Create numerical output fields\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(server_name=\"127.0.0.1\", server_port= 7860)"
      ],
      "metadata": {
        "id": "lLO8jT-lQ2wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# launch the application\n",
        "python3.11 gradio_demo.py"
      ],
      "metadata": {
        "id": "-zz1-qNyRDPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# click on the Web Application icon"
      ],
      "metadata": {
        "id": "icblks7WROtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Gradio application that can combine two input sentences together\n",
        "# gradio_demo.py\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def combine(a, b):\n",
        "    return a + \" \" + b\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=combine,\n",
        "    inputs = [\n",
        "        gr.Textbox(label=\"Input 1\"),\n",
        "        gr.Textbox(label=\"Input 2\")\n",
        "    ],\n",
        "    outputs = gr.Textbox(value=\"\", label=\"Output\")\n",
        ")\n",
        "demo.launch(server_name=\"127.0.0.1\", server_port= 7860)\n"
      ],
      "metadata": {
        "id": "i1uxQovgReVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Q&A bot using ibm watsonx.ai LLM\n",
        "# simple_llm.py\n",
        "# Import necessary packages\n",
        "from ibm_watsonx_ai.foundation_models import ModelInference\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai import Credentials\n",
        "from langchain_ibm import WatsonxLLM\n",
        "\n",
        "# Model and project settings\n",
        "model_id = 'mistralai/mixtral-8x7b-instruct-v01' # Directly specifying the model\n",
        "\n",
        "# Set necessary parameters\n",
        "parameters = {\n",
        "    GenParams.MAX_NEW_TOKENS: 256,  # Specifying the max tokens you want to generate\n",
        "    GenParams.TEMPERATURE: 0.5, # This randomness or creativity of the model's responses\n",
        "}\n",
        "\n",
        "project_id = \"skills-network\"\n",
        "\n",
        "# Wrap up the model into WatsonxLLM inference\n",
        "watsonx_llm = WatsonxLLM(\n",
        "    model_id=model_id,\n",
        "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "    project_id=project_id,\n",
        "    params=parameters,\n",
        ")\n",
        "\n",
        "# Get the query from the user input\n",
        "query = input(\"Please enter your query: \")\n",
        "\n",
        "# Print the generated response\n",
        "print(watsonx_llm.invoke(query))"
      ],
      "metadata": {
        "id": "GSkENR65R6Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# launch the Q&A bot\n",
        "python3.11 simple_llm.py"
      ],
      "metadata": {
        "id": "cRVgaoCSSPbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrate the application into Gradio\n",
        "touch ll_chat.py"
      ],
      "metadata": {
        "id": "wOj3Jj_RS1rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm_chat.py\n",
        "# Import necessary packages\n",
        "from ibm_watsonx_ai.foundation_models import ModelInference\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai import Credentials\n",
        "from langchain_ibm import WatsonxLLM\n",
        "import gradio as gr\n",
        "\n",
        "# Model and project settings\n",
        "model_id = 'mistralai/mixtral-8x7b-instruct-v01' # Directly specifying the model\n",
        "\n",
        "# Set necessary parameters\n",
        "parameters = {\n",
        "    GenParams.MAX_NEW_TOKENS: 256,  # Specifying the max tokens you want to generate\n",
        "    GenParams.TEMPERATURE: 0.5, # This randomness or creativity of the model's responses\n",
        "}\n",
        "\n",
        "project_id = \"skills-network\"\n",
        "\n",
        "# Wrap up the model into WatsonxLLM inference\n",
        "watsonx_llm = WatsonxLLM(\n",
        "    model_id=model_id,\n",
        "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "    project_id=project_id,\n",
        "    params=parameters,\n",
        ")\n",
        "\n",
        "# Function to generate a response from the model\n",
        "def generate_response(prompt_txt):\n",
        "    generated_response = watsonx_llm.invoke(prompt_txt)\n",
        "    return generated_response\n",
        "\n",
        "# Create Gradio interface\n",
        "chat_application = gr.Interface(\n",
        "    fn=generate_response,\n",
        "    allow_flagging=\"never\",\n",
        "    inputs=gr.Textbox(label=\"Input\", lines=2, placeholder=\"Type your question here...\"),\n",
        "    outputs=gr.Textbox(label=\"Output\"),\n",
        "    title=\"Watsonx.ai Chatbot\",\n",
        "    description=\"Ask any question and the chatbot will try to answer.\"\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "chat_application.launch(server_name=\"127.0.0.1\", server_port= 7860)"
      ],
      "metadata": {
        "id": "pGPuVaPnTMR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# launch the application\n",
        "python3.11 llm_chat.py"
      ],
      "metadata": {
        "id": "akf67GzZTP66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# click on the Web Application to launch the app"
      ],
      "metadata": {
        "id": "I7LapmzTTsu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "'''\n",
        "You might observe that the responses from the LLM are occasionally incomplete. Could you identify the cause of this issue? Also, would you be able to modify the code to enable the model to generate more content?\n",
        "'''\n",
        "# \"max_new_tokens\": 512,  # adjust the parameter `max_new_token` to a bigger value"
      ],
      "metadata": {
        "id": "bIkQEs5CT0_b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}